{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### numpy实现DNN梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.34445135]\n",
      " [-0.1168797 ]\n",
      " [-2.08005738]]\n",
      "[-1.47562099]\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]] [[0.02777032]\n",
      " [0.02478295]\n",
      " [0.97787599]\n",
      " [0.97520098]]\n"
     ]
    }
   ],
   "source": [
    "# 以下是一个简单的用numpy实现的手写梯度下降的神经网络的例子，其中用到了反向传播算法来更新权重和偏差，实现了对二分类问题的分类：\n",
    "import numpy as np\n",
    "\n",
    "# sigmoid函数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# sigmoid函数的导数\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# 输入数据\n",
    "X = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
    "# 输出数据\n",
    "y = np.array([[0, 0, 1, 1]]).T\n",
    "\n",
    "# 随机初始化权重和偏差\n",
    "np.random.seed(1)\n",
    "weights = 2 * np.random.random((3, 1)) - 1\n",
    "bias = 2 * np.random.random(1) - 1\n",
    "\n",
    "# 设置学习率\n",
    "learning_rate = 0.1\n",
    "\n",
    "# 设置训练次数\n",
    "num_epochs = 10000\n",
    "\n",
    "# 训练\n",
    "for epoch in range(num_epochs):\n",
    "    # 前向传播\n",
    "    inputs = np.dot(X, weights) + bias\n",
    "    outputs = sigmoid(inputs)\n",
    "\n",
    "    # 计算误差\n",
    "    error = y - outputs\n",
    "\n",
    "    # 计算梯度\n",
    "    gradients = error * sigmoid_derivative(outputs)\n",
    "\n",
    "    # 更新权重和偏差\n",
    "    weights += learning_rate * np.dot(X.T, gradients)\n",
    "    bias += learning_rate * np.sum(gradients)\n",
    "\n",
    "# 打印最终的权重和偏差\n",
    "print(weights)\n",
    "print(bias)\n",
    "print(y,outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  wide & deep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.3390 - accuracy: 0.1408\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 892us/step - loss: 1.2995 - accuracy: 0.1690\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.2611 - accuracy: 0.1972\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.2248 - accuracy: 0.2535\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.1895 - accuracy: 0.2817\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.1557 - accuracy: 0.3239\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.1235 - accuracy: 0.3803\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.0914 - accuracy: 0.4085\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.0605 - accuracy: 0.4507\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.0308 - accuracy: 0.4507\n",
      "Test accuracy: 0.444\n"
     ]
    }
   ],
   "source": [
    "# 分类\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load wine data\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the wide and deep parts of the model\n",
    "input_wide = tf.keras.layers.Input(shape=(7,))   #shape=(X_train.shape[1],)\n",
    "input_wide2 = tf.keras.layers.Dense(3)(input_wide)\n",
    "output_wide = input_wide2\n",
    "\n",
    "input_deep = tf.keras.layers.Input(shape=(6,))   #shape=(X_train.shape[1],)\n",
    "hidden = tf.keras.layers.Dense(units=64, activation='relu')(input_deep)\n",
    "output_deep = tf.keras.layers.Dense(units=3, activation='softmax')(hidden)\n",
    "\n",
    "\n",
    "# Merge the wide and deep parts of the model\n",
    "merged = tf.keras.layers.concatenate([output_wide, output_deep])\n",
    "output = tf.keras.layers.Dense(units=3, activation='softmax')(merged)\n",
    "merged_model = tf.keras.Model(inputs=[input_wide, input_deep], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "merged_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model to the training data\n",
    "merged_model.fit([X_train[:,6:], X_train[:,:6]], y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = merged_model.evaluate([X_test[:,6:], X_test[:,:6]], y_test, verbose=0)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "516/516 [==============================] - 1s 962us/step - loss: 0.6624 - mean_absolute_error: 0.5602\n",
      "Epoch 2/20\n",
      "516/516 [==============================] - 0s 903us/step - loss: 0.3835 - mean_absolute_error: 0.4327\n",
      "Epoch 3/20\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3428 - mean_absolute_error: 0.4141\n",
      "Epoch 4/20\n",
      "516/516 [==============================] - 0s 873us/step - loss: 0.3297 - mean_absolute_error: 0.3999\n",
      "Epoch 5/20\n",
      "516/516 [==============================] - 0s 899us/step - loss: 0.3077 - mean_absolute_error: 0.3888\n",
      "Epoch 6/20\n",
      "516/516 [==============================] - 0s 886us/step - loss: 0.3036 - mean_absolute_error: 0.3845\n",
      "Epoch 7/20\n",
      "516/516 [==============================] - 0s 884us/step - loss: 0.2937 - mean_absolute_error: 0.3754\n",
      "Epoch 8/20\n",
      "516/516 [==============================] - 0s 886us/step - loss: 0.2862 - mean_absolute_error: 0.3707\n",
      "Epoch 9/20\n",
      "516/516 [==============================] - 0s 878us/step - loss: 0.2942 - mean_absolute_error: 0.3688\n",
      "Epoch 10/20\n",
      "516/516 [==============================] - 0s 866us/step - loss: 0.2834 - mean_absolute_error: 0.3665\n",
      "Epoch 11/20\n",
      "516/516 [==============================] - 0s 872us/step - loss: 0.2741 - mean_absolute_error: 0.3617\n",
      "Epoch 12/20\n",
      "516/516 [==============================] - 0s 876us/step - loss: 0.2723 - mean_absolute_error: 0.3591\n",
      "Epoch 13/20\n",
      "516/516 [==============================] - 0s 891us/step - loss: 0.2700 - mean_absolute_error: 0.3561\n",
      "Epoch 14/20\n",
      "516/516 [==============================] - 0s 892us/step - loss: 0.2676 - mean_absolute_error: 0.3561\n",
      "Epoch 15/20\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.2679 - mean_absolute_error: 0.3530\n",
      "Epoch 16/20\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.2730 - mean_absolute_error: 0.3532\n",
      "Epoch 17/20\n",
      "516/516 [==============================] - 0s 872us/step - loss: 0.2596 - mean_absolute_error: 0.3489\n",
      "Epoch 18/20\n",
      "516/516 [==============================] - 0s 884us/step - loss: 0.2593 - mean_absolute_error: 0.3490\n",
      "Epoch 19/20\n",
      "516/516 [==============================] - 0s 895us/step - loss: 0.2549 - mean_absolute_error: 0.3463\n",
      "Epoch 20/20\n",
      "516/516 [==============================] - 0s 905us/step - loss: 0.2544 - mean_absolute_error: 0.3444\n",
      "129/129 [==============================] - 0s 555us/step - loss: 0.2774 - mean_absolute_error: 0.3581\n",
      "Test loss: 0.2774330973625183\n",
      "Test MAE: 0.3580664396286011\n"
     ]
    }
   ],
   "source": [
    "# 回归\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the California housing dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the input layers for the wide and deep parts of the model\n",
    "input_wide = tf.keras.layers.Input(shape=(X_train.shape[1],))\n",
    "input_deep = tf.keras.layers.Input(shape=(X_train.shape[1],))\n",
    "\n",
    "# Define the wide part of the model\n",
    "hidden_wide_1 = tf.keras.layers.Dense(units=32, activation='relu')(input_wide)\n",
    "hidden_wide_2 = tf.keras.layers.Dense(units=32, activation='relu')(hidden_wide_1)\n",
    "output_wide = tf.keras.layers.Dense(units=1, activation='linear')(hidden_wide_2)\n",
    "\n",
    "# Define the deep part of the model\n",
    "hidden_deep_1 = tf.keras.layers.Dense(units=64, activation='relu')(input_deep)\n",
    "hidden_deep_2 = tf.keras.layers.Dense(units=64, activation='relu')(hidden_deep_1)\n",
    "hidden_deep_3 = tf.keras.layers.Dense(units=64, activation='relu')(hidden_deep_2)\n",
    "output_deep = tf.keras.layers.Dense(units=1, activation='linear')(hidden_deep_3)\n",
    "\n",
    "# Merge the wide and deep parts of the model\n",
    "merged = tf.keras.layers.concatenate([output_wide, output_deep])\n",
    "output = tf.keras.layers.Dense(units=1, activation='linear')(merged)\n",
    "\n",
    "# Create the final model\n",
    "model = tf.keras.Model(inputs=[input_wide, input_deep], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit([X_train, X_train], y_train, epochs=20, batch_size=32)\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_mae = model.evaluate([X_test, X_test], y_test)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test MAE:', test_mae)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[ 0.04220146, -0.03328271, -0.04742723],\n",
       "       [ 0.04385703, -0.02046537, -0.0095141 ],\n",
       "       [ 0.014242  ,  0.00292666,  0.00830215]], dtype=float32)>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "import jieba\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "raw_text = '越努力越幸运'\n",
    "words = list(jieba.cut(raw_text))\n",
    "word_to_ix = {i:word for i,word in enumerate(set(words))} #索引化\n",
    "\n",
    "keys = word_to_ix.keys()\n",
    "keys_list = list(keys) # [0, 1, 2]\n",
    "\n",
    "embeds = tf.keras.layers.Embedding(4,3)\n",
    "embeds(tf.Variable(keys_list))\n",
    "\n",
    "# embeds = nn.Embedding(4,3)\n",
    "# embeds(torch.LongTensor(keys_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8490204427886767 0.15097955721132328\n",
      "(-30.56473594039236, 20.37649062692824)\n"
     ]
    }
   ],
   "source": [
    "#定义elo score 等级评分类\n",
    "class EloScore:\n",
    "    #初始积分\n",
    "    ELO_RATING_DEFAULT = 1500\n",
    "\n",
    "    #定义初始化方法\n",
    "    def __init__(self,Sa,ratingA=ELO_RATING_DEFAULT,ratingB=ELO_RATING_DEFAULT):\n",
    "        self.Sa = Sa\n",
    "        self.ratingA = ratingA\n",
    "        self.ratingB = ratingB\n",
    "      \n",
    "    #定义阈值 k值\n",
    "    def computeK(self,rating):\n",
    "        if rating >=2400:\n",
    "            return 16\n",
    "        elif rating >= 2100:\n",
    "            return 24\n",
    "        else:\n",
    "            return 36\n",
    "\n",
    "    #使用公式推算\n",
    "    def computeScore(self,):\n",
    "        Eb_S = 1 / (1+pow(10,(self.ratingA-self.ratingB)/400))  #B对A的胜概率\n",
    "        Ea_S = 1 - Eb_S #A对B的胜概率\n",
    "        return Ea_S,Eb_S\n",
    "    \n",
    "    def balanceK(self,):\n",
    "        avg_K = (self.computeK(self.ratingA) + self.computeK(self.ratingB))/2\n",
    "        return avg_K\n",
    "    \n",
    "    def main(self,):\n",
    "        K = self.balanceK()\n",
    "        Ea_S, Eb_S = self.computeScore()\n",
    "        print(Ea_S,Eb_S)\n",
    "        \n",
    "        Sa, Sb = self.Sa , 1 - self.Sa    # 实际胜负 WIN = 1, LOSS = 0, TIE = 0.5\n",
    "        \n",
    "        Ra = K * (Sa - Ea_S)\n",
    "        Rb = K * (Sb - Eb_S)\n",
    "        return Ra,Rb\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    eloscore = EloScore(0,1800,1500)\n",
    "    print(eloscore.main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.564735940392362"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "36*(0.5-0.8490204427886767)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.623413251903491"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow(10,(300)/400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15098897780462026"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/6.623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow(10,(800)/400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009900990099009901"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ffcde2d150310b1e1807dcd4cef0eee1065874d43f0e760c11aa9d0ad7dd1536"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
