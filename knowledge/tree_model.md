
### XGBoost参数
    XGBoost的作者把所有的参数分成了三类：
    1、通用参数：宏观函数控制。
    2、Booster参数：控制每一步的booster(tree/regression)。
    3、学习目标参数：控制训练目标的表现。

    一、通用参数
    1、booster[默认gbtree]
    选择每次迭代的模型，有两种选择：
    gbtree：基于树的模型
    gbliner：线性模型

    2、silent[默认0]
    当这个参数值为1时，静默模式开启，不会输出任何信息。
    一般这个参数就保持默认的0，因为这样能帮我们更好地理解模型。

    3、nthread[默认值为最大可能的线程数]
    这个参数用来进行多线程控制，应当输入系统的核数。
    如果你希望使用CPU全部的核，那就不要输入这个参数，算法会自动检测它。

    二、booster参数
    尽管有两种booster可供选择，我这里只介绍tree booster，因为它的表现远远胜过linear booster，所以linear booster很少用到。

    1、eta[默认0.3]
    和GBM中的 learning rate 参数类似。
    通过减少每一步的权重，可以提高模型的鲁棒性。
    典型值为0.01-0.2。

    2、min_child_weight[默认1]
    决定最小叶子节点样本权重和。
    和GBM的 min_child_leaf 参数类似，但不完全一样。XGBoost的这个参数是最小样本权重的和，而GBM参数是最小样本总数。
    这个参数用于避免过拟合。当它的值较大时，可以避免模型学习到局部的特殊样本。
    但是如果这个值过高，会导致欠拟合。这个参数需要使用CV来调整。

    3、max_depth[默认6]
    和GBM中的参数相同，这个值为树的最大深度。
    这个值也是用来避免过拟合的。max_depth越大，模型会学到更具体更局部的样本。
    需要使用CV函数来进行调优。
    典型值：3-10

    4、max_leaf_nodes
    树上最大的节点或叶子的数量。
    可以替代max_depth的作用。因为如果生成的是二叉树，一个深度为n的树最多生成n 2   n2个叶子。
    如果定义了这个参数，GBM会忽略max_depth参数。

    5、gamma[默认0]
    在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了节点分裂所需的最小损失函数下降值。
    这个参数的值越大，算法越保守。这个参数的值和损失函数息息相关，所以是需要调整的。

    6、max_delta_step[默认0]
    这参数限制每棵树权重改变的最大步长。如果这个参数的值为0，那就意味着没有约束。如果它被赋予了某个正值，那么它会让这个算法更加保守。
    通常，这个参数不需要设置。但是当各类别的样本十分不平衡时，它对逻辑回归是很有帮助的。
    这个参数一般用不到，但是你可以挖掘出来它更多的用处。

    7、subsample[默认1]
    和GBM中的subsample参数一模一样。这个参数控制对于每棵树，随机采样的比例。
    减小这个参数的值，算法会更加保守，避免过拟合。但是，如果这个值设置得过小，它可能会导致欠拟合。
    典型值：0.5-1

    8、colsample_bytree[默认1]
    和GBM里面的max_features参数类似。用来控制每棵随机采样的列数的占比(每一列是一个特征)。
    典型值：0.5-1

    9、colsample_bylevel[默认1]
    用来控制树的每一级的每一次分裂，对列数的采样的占比。
    我个人一般不太用这个参数，因为subsample参数和colsample_bytree参数可以起到相同的作用。但是如果感兴趣，可以挖掘这个参数更多的用处。

    10、lambda[默认1]
    权重的L2正则化项。(和Ridge regression类似)。
    这个参数是用来控制XGBoost的正则化部分的。虽然大部分数据科学家很少用到这个参数，但是这个参数在减少过拟合上还是可以挖掘出更多用处的。

    11、alpha[默认1]
    权重的L1正则化项。(和Lasso regression类似)。
    可以应用在很高维度的情况下，使得算法的速度更快。

    12、scale_pos_weight[默认1]
    在各类别样本十分不平衡时，把这个参数设定为一个正值，可以使算法更快收敛。

    三、学习目标参数
    这个参数用来控制理想的优化目标和每一步结果的度量方法。

    1、objective[默认reg:linear]
    这个参数定义需要被最小化的损失函数。最常用的值有：
        binary:logistic 二分类的逻辑回归，返回预测的概率(不是类别)。
        multi:softmax 使用softmax的多分类器，返回预测的类别(不是概率)。
        在这种情况下，你还需要多设一个参数：num_class(类别数目)。
        multi:softprob 和multi:softmax参数一样，但是返回的是每个数据属于各个类别的概率。

    2、eval_metric[默认值取决于objective参数的取值]
    对于有效数据的度量方法。
    对于回归问题，默认值是rmse，对于分类问题，默认值是error。典型值有：
        rmse 均方根误差(∑ N  i=1 ϵ 2  N    ‾ ‾ ‾ ‾ ‾ ‾  √   ∑i=1Nϵ2N)
        mae 平均绝对误差(∑ N  i=1 |ϵ| N     ∑i=1N|ϵ|N)
        logloss 负对数似然函数值
        error 二分类错误率(阈值为0.5)
        merror 多分类错误率
        mlogloss 多分类logloss损失函数
        auc 曲线下面积
    
    3、seed(默认0)
    随机数的种子
    设置    它可以复现随机数据的结果，也可以用于调整参数


### gbdt和xgboost区别

    1.GBDT是机器学习算法，而XGBoost是算法的工程实现
    2.使用CART作为基分类器时，XGBoost显式的加入了正则项来控制模型的复杂度，防止过拟合，提高了模型的泛化能力
    3.GBDT只使用了代价函数的一阶导数信息，而XGBoost对代价函数进行二阶泰勒展开，同时使用一阶和二阶信息
    4.传统的GBDT采用CART作为基分类器，而XGBoost支持多种类型的基分类器
    5.传统的GBDT迭代时采用全部的数据，而XGBoost采用了随机森林相似的策略，支持对数据进行 采样
    6.传统的GBDT没有对缺失值的处理策略，而XGBoost自动学习出对缺失值的处理策略


### xgboost

    核心思想：
    1.xgboost是属于boosting家族，是GBDT算法的一个工程实现
    2.在模型的训练过程中是拟合残差，在目标函数中使用了二阶泰勒展开并加入了正则
    3.在决策树的生成过程中采用了精确贪心的思路，寻找最佳分裂点的时候，使用了预排序算法 对所有特征都按照特征的数值进行预排序， 然后遍历所有特征上的所有分裂点位，计算按照这些候选分裂点分裂后的全部样本的目标函数增益，找到最大的那个增益对应的特征和候选分裂点位，从而进行分裂。这样一层一层的完成建树过程
    4.xgboost训练的时候，是通过加法的方式进行训练，也就是每一颗树通过拟合与真值的残差再训练一棵树出来，最后的预测结果是所有树的加和表示

    优点：
    1.高效可扩展。考虑了当数据量比较大，内存不够时怎么有效的使用磁盘，主要是结合多线程、数据压缩、分片的方法，尽可能的提高算法的效率
    2.鲁棒性强。相对于深度学习模型不需要精细调参便能取得接近的效果
    3.自动处理缺失值。当训练样本存在缺失值时，能自动学习分裂方向。如果训练无缺失值，而预测有缺失的话默认分到左子树
    4.防止过拟合。代价函数引入正则化项，控制了模型的复杂度，正则化项包含全部叶子节点的个数，每个叶子节点输出的score的L2模的平方和。降低了模型的方差，防止模型过拟合
    5.支持特征粒度并行。但并不是tree粒度上的，决策树最耗时的步骤是对特征的值排序，xgBoosting在迭代之前，先进行预排序，存为block结构，每次迭代，重复使用该结构，降低了模型的计算；block结构也为模型提供了并行可能，在进行结点的分裂时，计算每个特征的增益，选增益最大的特征进行下一步分裂，那么各个特征的增益可以开多线程进行
    6.支持列抽样。不仅能降低过拟合，还能减少计算​​​​​​​
    7.shrinkage（缩减），相当于学习速率（XGBoost中的eta）。XGBoost在进行完一次迭代时，会将叶子节点的权值乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间
    8.树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以XGBoost还提出了一种可并行的近似算法，用于高效地生成候选的分割点。近似算法首先根据特征分布的分位数提出了候选划分点，然后将连续型特征映射到由这些候选点划分的桶中(分桶)，然后聚合统计信息找到所有区间的最佳分裂点

    缺点：
    1.对cache优化不友好。在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的cache miss，降低算法效率
    2.虽然利用特征预排序和增益近似求解算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集，空间消耗大。这样的算法需要保存数据的特征值，还保存了特征排序的结果（例如排序后的索引，为了后续快速的计算分割点），这里需要消耗训练数据两倍的内存。 
    3.只适合处理结构化数据。相对于深度学习算法来说，XGBoost只适合处理结构化的特征数据，而对于类似图像中的目标检测等任务重的非结构化数据没有很好的处理能力
    4.不适合处理超高维特征数据。对于中低维数据具有很好的处理速度和精度，但是对于例如大规模图像物体识别，或者是推荐算法的某些场景中会出现的超高维特征的数据就无能为力了，这时候我们就需要借助于深度学习等算法

### lightGBM

[原理](https://zhuanlan.zhihu.com/p/99069186)
[详解](https://mp.weixin.qq.com/s/XxFHmxV4_iDq8ksFuZM02w)
[代码](https://github.com/Microstrong0305/WeChat-zhihu-csdnblog-code)

    优化：
    1.基于 Histogram 的决策树算法,直方图做差加速,基于直方图的稀疏特征优化
    2.单边梯度采样 Gradient-based One-Side Sampling(GOSS)：使用GOSS可以减少大量只具有小梯度的数据实例，这样在计算信息增益的时候只利用剩下的具有高梯度的数据就可以了，相比XGBoost遍历所有特征值节省了不少时间和空间上的开销。
    3.互斥特征捆绑 Exclusive Feature Bundling(EFB)：使用EFB可以将许多互斥的特征绑定为一个特征，这样达到了降维的目的。
    4.带深度限制的Leaf-wise的叶子生长策略：大多数GBDT工具使用低效的按层生长 (level-wise) 的决策树生长策略，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销。实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。LightGBM使用了带有深度限制的按叶子生长 (leaf-wise) 算法。
    5.直接支持类别特征(Categorical Feature)
    6.支持高效并行：特征并行、数据并行
    7.Cache命中率优化