一、Alex-NeT

1.提出端到端训练，在此之前都是将图片使用SIFT特征抽取进行训练

2.使用relu非线性激活函数（传统的为tanh、sigmoid）f(x)=max(0,x),简单即真理

3.将模型分到两块gpu上训练（模型并行，一块卡负责网络一部分参数的训练，即256图片一分为二），之后的六七年这个工程细节很没用，因为不可迁移，要是有多块gpu那切分起来很复杂，但是风水轮流转，bert出来后千亿万亿参数量的模型也训练不动了，又开始考虑到模型并行

4.防止过拟合方法：
（1）数据增强：将原始大小图片随即扣部分大小出来作为新样本；以及对于rgb三通道做了pca的分解方法
（2）dropout: 把多个模型融合起来是很有用的，但是对于神经网络来说很费劲，因为参数量大，训练麻烦，不如树模型集成方便。所以将隐藏层神经元随机失活（参数设为0），作者认为是相当于多个模型做了集成，但是后来人们发现并不是多模型做集成，其实就一个正则项，在线性模型上等价于L2正则，在更复杂的模型上等就是正则的效果。Dropout在rnn、attention用处很多

5.训练
（1）weight decay
是一种神经网络regularization的方法，它的作用在于让weight不要那么大，实践的结果是这样做可以有效防止overfitting。至于为什么有这样的效果，有些写书的人也说不清楚。

这是weight decay的公式，C就是Cost function。

C=C0+λ/2n*∑ww2

∂C/∂w=∂C0/∂w+λ/n*w

weight decay也被称为L2 regularization，或者是L2 parameter norm penalty。

我们可以从三个角度来理解weight decay是如何起作用的：
* 让weight变小一点，带来的好处是可以是整个神经网络对输入中的噪音（或者一点点变化）不要那么敏感；weight太大，其对应的输入的一点点变化就会起到主导作用，进而显著改变输出。
* 从公式来看，weight decay对于比较大的weight，decay的更多，比较小的weight，decay较小；这就相当于，weight越大，惩罚越大，即可以更有效的减少Cost函数。
* 让神经网络倾向于形成更简单的，“斜率”更小的模型；比如一个非线性模型，我们可以用很复杂的高阶多项式来表示，也可以容忍一些噪音，通过简单的低阶多项式来表示，甚至直接使用线性函数来表示。
最后，weight decay不是bias decay，不对bias起作用，原因可以这样来理解，如上第1点，bias不对应任何输入，虽然也就不存在导致网络敏感这样的问题了。

初始化权重：现在网络一般用均值为0，方差为0.01的高斯分布随机数初始化权重，就算是很深的bert，将方差调为0.02

(2)学习率下降

作者用的是验证集上误差不在下降就手动调低10倍的学习率，之后几年的主流做法都是自动的过三十轮下降10倍，在最近的话都是先学习率由低到高一个直线，在一个cos函数曲线下降（warm-up）