###  1.摘要
序列转义模型（机器翻译，自然语言处理等）一般是用复杂的循环或者是卷积神经网络包含encoder和decoder来实现，比较好的这些模型通常会用注意力机制在encoder和decoder之间。本文提出了一个简单的架构仅仅依赖于attention机制，而没有使用循环和卷积，针对机器翻译任务达到了简单快速的sota，而后应用于语音、视频、图片等地方也很有效，火出圈。
### 2.引言
时序模型需要得到前一步的输出作为输入，所以并行度很差，时间开销大，而且针对序列长的数据记忆性也不好，就算将记忆量增大，但是内存开销也会更大。
Attention在此之前已经成功用于编码器和解码器，用于有效的将编码器数据传给解码器，本文提出的transformer更好。
### 3.背景
提了几个前人工作将卷积网络替换掉循环网络，这些工作对于长序列难以建模，因为卷积每次计算都是看的感受野的部分，所以没有全局信息，所以都是用很多层卷积，将隔着很远的数据点给融合起来。但是如果使用transformer的注意力机制，每一次都能看到所有的数据，只需要一层网络，又提到卷积的好处是可以做多个输出通道，每个通道可以说是识别不同的模式，作者也想要多通道输出的效果，所以提出了一个多头的注意力机制。
### 4.模型架构
#### 4.1序列转义模型常用编码器和解码器架构。
编码器就是比如将长度为t的句子，编码长度也为t，表示对应的第t个词的向量表示，这就是编码器的输出，原始输入变为了机器学习可以理解的向量。

解码器会拿到解码器输出，生成长度为m的序列（t和m可一样可不一样长），解码器有个不同点是编码器基本上一次可以看到原始的整个句子，一次就把向量生成了，但是解码器的向量只能一个一个生成的，这个叫做自回归模型，就是我的输出又是我的输入，也就是解码器生成第一个输出值后，又可以根据第一个输出值生成第二个输出，也就是翻译是一个词一个词往外生成。

架构介绍：
编码器用的是6个完全一样层，每个层有两个子层：多头自注意力层和MLP层，这两个子层间都用了残差链接，由于残差连接需要将输入输出一样大小，所以每个词在每一层都是512维度的表示。这6个层完了就是接一个layer normalization。为啥不用 batch normalization?在变长的应用里面一般不使用batch norm。

如下图所示。假设有个二维输入，也就是矩阵，每行是个样本特征，行数就是一个batch，batch norm就是把每一列特征数据（列向量）变成均值为零，方差为1。如何将列向量变为这个分布呢？

也就是将向量减去自己的均值，然后除以方差。训练时候是小批量，每一批每一列算一个这个batch norm，预测时候一般会整个数据扫一遍，也就是在所有样本数据的列上的均值和方差。除此之外，bn还会学一个拉姆达和贝塔，可以把这个向量通过学习转换成任意方差、均值为某个值的东西。Layer norm是对每个样本（每个行均值为0，方差为1）这样子干，可以认为是将数据转置一下放到bn里面，然后结果再转至回去就得到自己的结果了，这只是二维的情况。

在我们的transformer里面或者是正常的rnn里，他的输入是三维的东西。因为序列是由一段词，这是一维度，每个词还有对应的向量表示这是二维度，还有个batch对应三维度，序列sequence的长度就是n，特征feature也就是每个词向量的长度为d（transformer设为了512），蓝色线就是bn切法，黄色就是ln切法。

<img align="center"  width='200' height='300' src="picture/transformer1.png"  />

为什么时序序列模型中ln用的多？因为样本长度会发生变化，这两种切法如下所示，对于计算均值和方差有不同影响。Bn是特征维度，取了batch个样本的某个特征计算均值方差，由于样本长度有长有短，小批量时算出的均值方差波动较大，另外一个问题是做预测时，要把全局的均值和方差记录下来， 如果遇到一个很长的样本训练时没见过，所以之前的计算的均值和方差就没那么好用。相对于来说ln没有上述问题，因为是每个样本算自己的均值和方差，也不需要存下全局的均值和方差，因为是自己样本算均值方差，比较稳定（但是这样是也是来一个很长的样本，不也是比较麻烦吗？）。上述都是作者给的解释，实际上之后ln有效的解释时对于梯度，输入的那些norm来提升lipsics常数来解释的。

<img align="center"  width='200' height='300' src="picture/transformer2.png"  />

解码器：有六个一样的层组成，包含编码器的两个子层，以及一个特有的子层：带掩码的多头注意力机制。解码器做的是自回归，当前时刻输出是下一时刻输入，意味着做预测不能看到之后的那些时刻的输出，但因为在注意力机制在每一个时刻都可看到完整输入，所以要避免这个情况，所以训练预测要避免。

#### 4.2Attention
注意力函数是将一个query和一些key-value对映射成输出的一个函数，这里的query、key、value、output都是一些向量，具体来说，output是value的加权和，因此输出维度和value是一样的，对于每一个value的权重是value对应的key和查询的query的相似度（很多算法）计算来的。（通俗就是查词典：一个查询过来，query和key计算相似度，那么越相似的key对应的value的权重就越大，输出就是每个value权重*对应value的向量）

4.2.1点积注意力机制
文中用的注意力机制是比较简单的，用的query和key是等长的dk,可以不等长，用其他办法算，value是dv，输出也就是dv，具体计算就是query和每个key两两做内积，值越大相似度越高，如果为0就是正交，也就是无相似度。算出的值再除以根号下dk，然后再用softmax得到每个value对应的权重，相乘求和就得到输出了。

实际中一个个算太慢，有n个（行）dk维度的query，有m个（行）dk维度的key，必须都dk才可以内积（对应元素相乘），两个相乘得到了n*m（每行代表着一个query对所有key的内积值），再除以根号下dk，再对每行做softmax 得到权重。V是m行dv列的矩阵，权重和V相乘得到n*dv,每一行即我们需要的输出

<img align="center"  width='800' height='200' src="picture/transformer3.png"  />

一般有两种注意力机制。加性注意力机制可以处理Q和K不同维度的情况，另一种就是点积的注意力机制，点积和文章使用的是一致的，只不过作者多除了个根号下dk 。当维度dk不大时，除不除都可以，当很大时候，值之间的相对差距就会变大，也就是最大的值更加靠近于1，其他靠近于0，这样的情况算梯度发现梯度很小，就会跑不动。（所以他设置了dk=512维度，除以根号下dk是个好选择）
4.2.2如何做mask

Mask主要避免在第t时刻看到t及以后的输入，第t时刻的Qt，在做计算时候只能看K1~Kt-1 ，后面的K看不到， 计算的时候将后面的值设一个很大负数（如1e-10），这个大的负数进入softamax做指数运算的时候就会变为0，所以t及t时刻以后的权重都为0

#### 4.3多头注意力机制
如下图所示，原始的VKQ经过一个线性层投影到一个较低的维度，再做一个上述的内积的attention，做h次 得到h个输出向量，按顺序拼接h个向量，就是学习h个不同的投影方式，再线性输出（即如下述公式，投影到Wo），对于每一个头通过不同的可学习的权重W投影到低维，再做上述的attention

<img align="center"  width='800' height='200' src="picture/transformer4.png"  />

<img align="center"  width='500' height='600' src="picture/transformer5.png"  />

#### 4.4多头注意力的三种使用

<img align="center"  width='400' height='600' src="picture/transformer6.png"  />

如图，黄色部分就是三个多头注意使用，首先从编码器输入序列编码以及位置编码的信息，进入多头注意力，这里的同一个输入分成三份QKV，也就是自注意力，自己和自己算相似度得到权重乘以自己得到输出，即输入的加权和。

解码器的第一个mask多头也是同理，额外多了个我们前面提到的mask，计算t时刻t及其后面的权重设为0。
解码器上面那个不再是多头自注意力了，因为如图QKV不一样，从左到右三个依次是编码器输出作为KV，Q是解码器的第一个多头注意力的输出，对于解码器的每一个Q，算一个来自编码器输出V的加权和。
总结：根据在解码器输入序列向量的不一样，在编码器输出中挑选感兴趣的东西。
#### 4.5 feedforward 
Position-wise FFN。输入的序列有很多词，position就是每个词，同一个MLP对每个次作用一次（沐神说最后一个维度做计算是啥意思）。

<img align="center"  width='500' height='100' src="picture/transformer7.png"  />

中间函数是一个全连接层（投影成2048长度），relu激活（max）。然后再接一个全连接回到512。
####4.6  transformer块和rnn作用于序列数据的区别
左transformer: 红色长方形为attention    
右rnn

<img align="center"  width='200' height='300' src="picture/transformer8.png"  />

#### 4.7 position encoding
因为attention的输出是每个value乘以对应权重的向量，但是会丢失位置信息，这样的话不同顺序的相同词序列都是同一个输出，所以需要把时序信息放入。
参考计算存储一个字符或汉字是靠32位bit存储的，同理我们的一个序列的词用512维的向量表示，再拿一个512维向量表示位置，将两个向量相加即为输入，具体通过下述公式

<img align="center"  width='400' height='100' src="picture/transformer9.png"  />


### 5.为什么使用自注意力

<img align="center"  width='700' height='300' src="picture/transformer10.png"  />

### 7.结论
提出使用多头自注意力机制的encoder和decoder架构，弃用rnn和cnn，效果很好。我们对使用纯注意力机制的任务很激动，想用于其他任务